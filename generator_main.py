# -*- coding: utf-8 -*-
"""Generator main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RIzkAgiarwio6s1a0weAFskHfKbDyNd7
"""

import os
from PIL import Image
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Input, Dense, BatchNormalization, Activation, Reshape, Flatten, Dropout, Concatenate
from keras.layers import LeakyReLU, PReLU
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.models import Model
from tqdm import tqdm
import tensorflow as tf

# Define the image folder path
image_folder = '/content/data/train data'

# Set the desired input size of the model
input_size = (256, 256, 3)

# Set the desired output size of the model
output_size = (256, 256, 3)

# Define an empty list to store the images
images = []
resized_images = []

# Define the data augmentation parameters
datagen = ImageDataGenerator(
    rotation_range=20,  # Random rotation between -20 and 20 degrees
    width_shift_range=0.1,  # Randomly shift the width by 10%
    height_shift_range=0.1,  # Randomly shift the height by 10%
    shear_range=0.2,  # Shear angle in counter-clockwise direction
    zoom_range=0.2,  # Randomly zoom by 20%
    horizontal_flip=True,  # Randomly flip images horizontally
    fill_mode='nearest'  # Fill mode for newly created pixels
)

# Loop through the image folder and load the images
for filename in tqdm(os.listdir(image_folder)):
    try:
        # Check if the file is a valid image file
        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):
            # Load the image using Pillow
            img = Image.open(os.path.join(image_folder, filename))

            # Resize the image to the desired input size
            img_input = img.resize(input_size[:2])

            # Convert the image to a numpy array and normalize pixel values
            img_input_array = np.array(img_input) / np.float32(256.0)

            # Add the input image to the list
            images.append(img_input_array)

            # Resize the image to the desired output size
            img_output = img.resize(output_size[:2])

            # Convert the resized image to a numpy array and normalize pixel values
            img_output_array = np.array(img_output) / np.float32(256.0)

            # Add the output image to the list
            resized_images.append(img_output_array)

            # Apply data augmentation to the input image and add augmented images to the list
            img_input_augmented = datagen.random_transform(img_input_array)
            images.append(img_input_augmented)

            # Apply data augmentation to the output image and add augmented images to the list
            img_output_augmented = datagen.random_transform(img_output_array)
            resized_images.append(img_output_augmented)

    except:
        # Skip the file if there is an error
        print(f"Error loading image file: {filename}")
        continue

# Convert the lists of images to numpy arrays
images = np.array(images)
resized_images = np.array(resized_images)


# Define the denoiser model
def create_denoiser_model(input_shape):
    # Define the input layer
    input_layer = Input(shape=input_shape)

    # Encoder
    enc_layer1 = Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(input_layer)
    enc_layer1 = BatchNormalization()(enc_layer1)
    enc_layer1 = LeakyReLU(alpha=0.2)(enc_layer1)
    enc_layer1 = Dropout(0.5)(enc_layer1)  # Dropout regularization

    enc_layer2 = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(enc_layer1)
    enc_layer2 = BatchNormalization()(enc_layer2)
    enc_layer2=LeakyReLU(alpha=0.2)(enc_layer2)
    enc_layer2 = Dropout(0.5)(enc_layer2)  # Dropout regularization

    enc_layer3 = Conv2D(256, kernel_size=3, strides=2, padding='same', activation='relu')(enc_layer2)
    enc_layer3 = BatchNormalization()(enc_layer3)
    enc_layer3=LeakyReLU(alpha=0.2)(enc_layer3)
    enc_layer3 = Dropout(0.5)(enc_layer3)  # Dropout regularization

    enc_layer4 = Conv2D(512, kernel_size=3, strides=2, padding='same', activation='relu')(enc_layer3)
    enc_layer4 = BatchNormalization()(enc_layer4)
    enc_layer4=LeakyReLU(alpha=0.2)(enc_layer4)
    enc_layer4 = Dropout(0.5)(enc_layer4)  # Dropout regularization

    enc_layer5 = Conv2D(512, kernel_size=3, strides=2, padding='same', activation='relu')(enc_layer4)
    enc_layer5 = BatchNormalization()(enc_layer5)
    enc_layer5=LeakyReLU(alpha=0.2)(enc_layer5)
    enc_layer5 = Dropout(0.5)(enc_layer5)  # Dropout regularization

    enc_layer6 = Conv2D(512, kernel_size=3, strides=2, padding='same', activation='relu')(enc_layer5)
    enc_layer6 = BatchNormalization()(enc_layer6)
    enc_layer6=LeakyReLU(alpha=0.2)(enc_layer6)
    enc_layer6 = Dropout(0.5)(enc_layer6)  # Dropout regularization

    # Decoder
    dec_layer1 = Conv2DTranspose(512, kernel_size=4, strides=2, padding='same')(enc_layer6)
    dec_layer1 = BatchNormalization()(dec_layer1)
    dec_layer1 = LeakyReLU(alpha=0.2)(dec_layer1)

    dec_layer2 = Concatenate()([dec_layer1, enc_layer5])
    dec_layer2 = Conv2DTranspose(512, kernel_size=4, strides=2, padding='same')(dec_layer2)
    dec_layer2 = BatchNormalization()(dec_layer2)
    dec_layer2 = LeakyReLU(alpha=0.2)(dec_layer2)

    dec_layer3 = Concatenate()([dec_layer2, enc_layer4])
    dec_layer3 = Conv2DTranspose(256, kernel_size=4, strides=2, padding='same')(dec_layer3)
    dec_layer3 = BatchNormalization()(dec_layer3)
    dec_layer3 = LeakyReLU(alpha=0.2)(dec_layer3)

    dec_layer4 = Concatenate()([dec_layer3, enc_layer3])
    dec_layer4 = Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(dec_layer4)
    dec_layer4 = BatchNormalization()(dec_layer4)
    dec_layer4 = LeakyReLU(alpha=0.2)(dec_layer4)

    dec_layer5 = Concatenate()([dec_layer4, enc_layer2])
    dec_layer5 = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(dec_layer5)
    dec_layer5 = BatchNormalization()(dec_layer5)
    dec_layer5 = LeakyReLU(alpha=0.2)(dec_layer5)

    dec_layer6 = Concatenate()([dec_layer5, enc_layer1])
    dec_layer6 = Conv2DTranspose(32, kernel_size=4, strides=2, padding='same')(dec_layer6)
    dec_layer6 = BatchNormalization()(dec_layer6)
    dec_layer6 = LeakyReLU(alpha=0.2)(dec_layer6)

    output_layer = Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')(dec_layer6)



    # Define the generator model
    model = Model(inputs=[input_layer], outputs=[output_layer], name='generator')
    model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),
              metrics=['accuracy'])
    return model

denoiser_model = create_denoiser_model((256, 256, 3))
denoiser_model.summary()




# Train the denoiser model
denoiser_model.fit(resized_images, resized_images, epochs=5, batch_size=16)


# Save the model to a file
model_filename = 'denoiser_model.h5'
denoiser_model.save(model_filename)
print("Model saved to:", model_filename)